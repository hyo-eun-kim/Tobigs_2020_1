## introduction to ensemble

 ![bias_variacne](./image/1.jpg)
 
**ì—ëŸ¬ëŠ” biasì™€ varianceë¡œ êµ¬ì„±ì´ ëœë‹¤.**  
**bias**ëŠ” ì‹¤ì œ valueì™€ ì˜ˆì¸¡ valueì™€ì˜ ì°¨ì´ê°€ ì–´ëŠ ì •ë„ë˜ëŠ” ì§€ íŒŒì•…í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.  
high biasëŠ” underfitting ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.  
**variance**ëŠ” quantifies how are the prediction made on same observation different from each other.  
high varianceëŠ” overfitting ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤.  
![bias_variacne](./image/2.jpg) 


biasì™€ variance ì‚¬ì´ì—ëŠ” trade-off ê´€ê³„ê°€ ìˆê¸° ë•Œë¬¸ì—, 
ëª¨ë¸ì€ ë°˜ë“œì‹œ ë‘ ì¢…ë¥˜ì˜ error(bias, variance) ì‚¬ì´ì˜ ê· í˜•ì„ ìœ ì§€í•´ì•¼ í•œë‹¤.   
(=biasì™€ varianceì˜ í•©ì´ ê°€ì¥ ì‘ì€ ì§€ì ì´ optimal pointê°€ ëœë‹¤.)
**ensemble learning is one way to execute this bias-variance trade off analysis.** 
 
ì—¬ê¸°ì„œ ensemble learningì€ ë¬´ì—‡ì¸ê°€? ensembleì€ group of predictorsì„ ì´ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. 
ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¤ìˆ˜ì—ê²Œ ë¬¼ì–´ë³´ê³ , ê·¸ë“¤ì˜ ê²°ê³¼ë¥¼ aggregateí•œë‹¤ê³  ê°€ì •í•˜ì. ì´ëŸ¬í•œ ê²½ìš°ì— aggregatedëœ ë‹µì´ í•œ ì „ë¬¸ê°€ì˜ ë‹µë³€ë³´ë‹¤ ë” ë‚«ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ê²ƒì„ êµ°ì¤‘ì˜ ì§€í˜œë¼ê³  ë¶€ë¥´ëŠ”ë° ì´ì™€ ìœ ì‚¬í•˜ê²Œ **ì—¬ëŸ¬ ê°œì˜ ì˜ˆì¸¡ ëª¨ë¸ì˜ predictionì„ aggregate**í•œë‹¤ê³  ê°€ì •í•œë‹¤ë©´ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° best individual predictorì˜ ê²°ê³¼ë³´ë‹¤ ë” ì¢‹ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ê²Œ ë  ê²ƒì´ë‹¤.
  
  
_types of ensembling_
- basic ensemble technique
    - max voting (hard voting, soft voting + model weight)
    - averaging
    - weighted average
- advanced ensemble technique
    - stacking
    - bagging
    - boosting
   
   
   
ì•™ìƒë¸”
ë°ì´í„° ê°’ì„ ì˜ˆì¸¡í•  ë•Œ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ì¡°í™”ë¡­ê²Œ í•™ìŠµì‹œì¼œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì´ìš©í•˜ì.
-> ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ëª¨ë¸ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ê¸°ë²•
-> ì—¬ëŸ¬ ê°œì˜ weak learnerë¥¼ ê²°í•©í•˜ì—¬ strong classifierë¥¼ ë§Œë“œëŠ” ê²ƒ

---
â€ƒ
## Ensemble â€“ (1) Voting

1. > Max voting (soft voting, hard voting)

    max votingì€ ì¼ë°˜ì ìœ¼ë¡œ classificationì— ì‚¬ìš©ëœë‹¤. 
    multiple modelì€ ê° dataì— ëŒ€í•´ì„œ ì˜ˆì¸¡ì„ í•˜ëŠ”ë°, ì´ ì˜ˆì¸¡ì€ â€˜voteâ€™ì²˜ëŸ¼ ì—¬ê²¨ì§„ë‹¤. 
    í•œ ë°ì´í„°ì— ëŒ€í•œ ì—¬ëŸ¬ ê°œì˜ predictionsë¥¼ ë‹¤ìˆ˜ê²°ì— ë”°ë¼ final predictionìœ¼ë¡œ ê²°ì •í•œë‹¤.
    ex) ì–´ë–¤ valueì— ëŒ€í•œ prediction ê°’ìœ¼ë¡œ [5, 4, 5, 4, 4]ê°€ ë‚˜ì™”ë‹¤ë©´ final predictionì€ 4ê°€ ëœë‹¤.

    **votingì— ëŒ€í•œ ë°©ë²•ì€ hard votingê³¼ soft votingì´ ìˆëŠ”ë°**, ìœ„ì˜ ì˜ˆì‹œì™€ ê°™ì´ ë‹¨ìˆœ ë‹¤ìˆ˜ê²°ì˜ ì›ì¹™ì— ë”°ë¼ ìµœì¢… ì˜ˆì¸¡ ê°’ì„ ê²°ì •í•˜ëŠ” ë°©ë²•ì„ hard voting, ê° class ë³„ ì˜ˆì¸¡ëœ í™•ë¥ ì„ ì´ìš©í•˜ë©´ soft votingì´ë¼ê³  í•œë‹¤.
    ëŒ€ë¶€ë¶„ì˜ hard votingë³´ë‹¤ soft votingì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì€ë°, ê·¸ ì´ìœ ëŠ” confident voteì— ë” weightë¥¼ ë¶€ì—¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (í•˜ì§€ë§Œ soft voting ë°©ë²•ì€ classifierê°€ well-calibratedëœ ê²½ìš°ì— ì‚¬ìš©í•˜ê¸¸ ì¶”ì²œí•œë‹¤)
    ex) C1(x) = [0.9, 0.1], C2(x) = [0.8, 0.2], C3(x)=[0.4, 0.6]
    P(Y0|X)=(0.9+0.8+0.4)/3=0.7, P(Y1|X)=(0.1+0.2+0.6)/3=0.3 -> Y0ì´ë¼ê³  ì˜ˆì¸¡
    + ì¶”ê°€ë¡œ modelì— ëŒ€í•œ weightë„ ë¶€ì—¬í•  ìˆ˜ ìˆë‹¤. (ì„±ëŠ¥ì´ ë¹„êµì  ë–¨ì–´ì§€ëŠ” modelì—ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë‚®ê²Œ)
    
    
2. > averaging (same weight)

    **ì´ ë°©ë²•ì€ modelì˜ predictionì„ averageí•˜ì—¬ ìµœì¢… predicted valueë¡œ ê²°ì •í•˜ëŠ” ê²ƒì´ë‹¤.**
    averaging ë°©ë²•ì€ regressionì—ì„œ predictionì„ í•  ë•Œ, ë˜ëŠ” classificationì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤.
    (regressionì—ì„œ ì‚¬ìš©í•  ë•ŒëŠ” ì˜ˆì¸¡ëœ ê°’ë“¤ì„ í‰ê·  ë‚´ì–´ ìµœì¢… ì˜ˆì¸¡ ê°’ìœ¼ë¡œ ê²°ì •í•˜ëŠ” ë°©ì‹,
    classificationì—ì„œëŠ” predict_proba()ì—ì„œ ë‚˜ì˜¨ outputë“¤ì„ í‰ê·  ë‚´ì–´ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©ë¨)

3. weighted average (different weight)
    **ì´ ë°©ë²•ì€ averaging ë°©ë²•ì˜ í™•ì¥ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.**
    ëª¨ë¸ì€ ëª¨ë¸ì˜ ì¤‘ìš”ë„ì— ë”°ë¼ **ë‹¤ë¥¸ weight**ë¥¼ ë°›ê²Œ ëœë‹¤. (ì„±ëŠ¥ ì¢‹ì„ìˆ˜ë¡ weightë¥¼ ë” ë¶€ì—¬)
    (ê·¸ëƒ¥ average ë°©ë²•ì€ ëª¨ë¸ë³„ë¡œ ë™ì¼í•œ weightë¥¼ ë°›ì€ ê²ƒì´ë‹¤.)

---

## Ensemble â€“ (2) Bagging

baggingì€ competitionì—ì„œ ë§¤ìš° ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì´ë‹¤.
(ì´ ê¸€ì˜ ì €ìëŠ” ì´ê²ƒì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëŒ€íšŒì—ì„œ ì´ê¸°ëŠ” ê²ƒì„ ë³¸ ì ì´ ì—†ì„ ì •ë„ë¼ê³  í•œë‹¤.)
ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ **data must have variance.**
> in order for this to work, your data must have variance, otherwise youâ€™re just adding levels after levels of additional iterations with little benefit to your score and a big headache for those maintaining your modeling pipeline in production

bagging irons out variance from a data set.
**ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆˆ í›„, training í•  ë•Œ predictionì´ ë‹¤ë¥´ë‹¤ë©´ dataëŠ” varianceê°€ ìˆëŠ” ê²ƒì´ë‹¤.**

ì´ë ‡ê²Œ ì—¬ëŸ¬ ê°œì˜ predictionì„ ensemble(averaging, voting)í•˜ê²Œ ë˜ë©´, 
1. biasì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³  varianceë¥¼ ë‚®ì¶œ ìˆ˜ ìˆìœ¼ë©° (=overfitting ë°©ì§€)
2. accuracy ë˜í•œ ì¦ê°€
3. predictionì„ ì•ˆì •í™”
í•˜ì§€ë§Œ ì´ ê°€ì •ì€ ëª¨ë‘ **dataê°€ varianceë¥¼ ê°–ê³  ìˆë‹¤ëŠ” ê°€ì •** í•˜ì—ì„œ ì„±ë¦½í•œë‹¤. 
ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ baggingì€ ë„ì›€ì´ ë˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.
  

> 

1. **ë³µì›ì¶”ì¶œ**ì„ í•˜ë©° ì—¬ëŸ¬ ê°œì˜ datasetsì„ ë§Œë“ ë‹¤.
2. ë³µì›ì¶”ì¶œí•´ì„œ ë§Œë“  dataset ê°ê° ë§ˆë‹¤ modelì„ ì ìš©í•œë‹¤.
3. ê°ê°ì˜ modelì€ independentí•˜ë‹¤.
4. ê°ê°ì˜ modelì—ì„œ X_testë¥¼ predictí•œë‹¤.
5. ê·¸ **predictionsë¥¼ aggregate** í•˜ì—¬ final predictionì„ ë§Œë“ ë‹¤. (votingí•˜ê±°ë‚˜ averaging í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ)
+ ì´ ë•Œ ê°ê°ì˜ modelì€ high variance(=overfitting) ëª¨ë¸ì´ ì í•©í•˜ë‹¤. (aggregateí•˜ë©´ generalí•˜ê²Œ ë¨)
+ ê°ê°ì˜ modelì€ ë™ì¼í•œ ëª¨ë¸ì´ë‹¤.

+ bootstrapping
ë°ì´í„°ë¥¼ ì™¸ë¶€ ì¶”ê°€ ì—†ì´ ë³µì› ì¶”ì¶œí•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ê²ƒ
ë³µì›ì¶”ì¶œ í•˜ë‹¤ë³´ë©´ í¬í•¨ë˜ì§€ ì•Šì€ ë°ì´í„°ê°€ ìƒê¸´ë‹¤ 
-> ì´ ë°ì´í„°ë¥¼ validation dataë¡œ ì´ìš©í•´ OOB-error ê³„ì‚°
(ê° ëª¨ë¸ë§ˆë‹¤ OOB dataê°€ ë‹¤ë¥¸ ê²ƒ)

 !["bagging"](./image/3.jpg)
 

> 1. Bagging - bagging meta-estimator

ì´ ë°©ë²•ì€ classificationê³¼ regression ëª¨ë‘ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.
bagging meta-estimatorì˜ stepì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
1. bootstrappingì„ ì´ìš©í•´ random subsetsê°€ ë§Œë“¤ì–´ì§„ë‹¤. (ë³µì›ì¶”ì¶œ)
2. ì´ **random subsetsëŠ” ëª¨ë“  featuresë¥¼ í¬í•¨í•œë‹¤.**
3. base estimatorê°€ ê° subsetsì— fittedëœë‹¤. (ë™ì¼í•œ modelì´ ì ìš©ë˜ëŠ” ê²ƒ)
4. ê° modelë¡œë¶€í„°ì˜ predictionsëŠ” combinedëœë‹¤.

> 2. Bagging â€“ Random Forest (ì‰½ê²Œ ìƒê°í•˜ë©´ bootstrapping + decision tree)

random forestì—ì„œ **base estimatorëŠ” decision tree**ì´ë‹¤.
bagging meta-estimatorì™€ ë‹¬ë¦¬, random forestëŠ” **randomly selects a set of features.**
1. bootstrappingì„ ì´ìš©í•´ random subsetsê°€ ë§Œë“¤ì–´ì§„ë‹¤. (ë³µì›ì¶”ì¶œ)
2. decision treeì—ì„œ ê° nodeì—ì„œ best splitì„ ê²°ì •í•˜ê¸° ìœ„í•´ random set of featuresê°€ ê³ ë ¤ëœë‹¤.
3. decision treeëŠ” ê° subsetsë§ˆë‹¤ fittedëœë‹¤. 
4. ê° modelë¡œë¶€í„° predictionsê°€ averagingë˜ì–´ final predictionì´ ê²°ì •ëœë‹¤.

Note: The decision trees in random forest can be built on a subset of data and features. Particularly, the sklearn model of random forest uses all features for decision tree and a subset of features are randomly selected for splitting at each node.


< bagging ì¤‘ í•˜ë‚˜ì¸ Random Forestì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…>
 !["RF"](./image/4.jpg)
 
1. ê° ë°ì´í„°ëŠ” bootstrap ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ì¶”ì¶œëœë‹¤.
2. decision tree êµ¬ì¶• ì‹œ ë³€ìˆ˜ë¥¼ random selection
ì›ë³¸ ë°ì´í„°ì—ì„œ featureê°€ nê°œì˜€ë‹¤ê³  ìƒê°í•˜ì.
ìš°ì„  ì²« ë²ˆì§¸ branch splitì„ ìœ„í•˜ì—¬ sqrt(n)ê°œì˜ featureë¥¼ ëœë¤ ì¶”ì¶œí•œë‹¤.
gini index í˜¹ì€ entropy ê³„ì‚°í•˜ì—¬ ë¶ˆìˆœë„ê°€ ë‚®ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ = information gainì´ ë†’ì€ ë°©í–¥ìœ¼ë¡œbranch splitì„ ì§„í–‰í•œë‹¤.
ê·¸ ë‹¤ìŒ ë‹¤ì‹œ sqrt(n)ê°œì˜ featureë¥¼ ëœë¤ ì¶”ì¶œí•˜ì—¬ branch splití•˜ê³ ,
ë‹¤ì‹œ sqrt(n)ê°œì˜ feature ëœë¤ ì„ íƒí•˜ì—¬ branch split í•˜ëŠ” â€¦ full-grown treeê°€ ë  ë•Œê¹Œì§€ ë°˜ë³µ

+ decision treeëŠ” ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ”ë°,
ê°ê°ì˜ featureê°€ ë¶ˆìˆœë„ë¥¼ ì–¼ë§ˆë‚˜ ê°ì†Œì‹œí‚¤ëŠ” ì§€ í‰ê·  ê³„ì‚°í•˜ì—¬ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•œë‹¤.
(=ë¶ˆìˆœë„ë¥¼ ë§ì´ ê°ì†Œì‹œí‚¬ìˆ˜ë¡ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ê°€ ë†’ì•„ì§„ë‹¤.)
+ ê·¼ë° ì—¬ê¸°ì„œ featureì˜ classê°€ ë†’ì„ìˆ˜ë¡ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ê°€ ë†’ì•„ì§€ëŠ” ê²½í–¥ (ë³€ìˆ˜ ì¤‘ìš”ë„ ì™œê³¡ â˜¹)
+ ë˜í•œ ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ê°€ selection ë˜ë©´ ìƒê´€ê´€ê³„ ë†’ì•˜ë˜ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì˜ ì¤‘ìš”ë„ëŠ” ë‚®ì•„ì§ (ì—­ì‹œ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„ ì™œê³¡ â˜¹ )

---

## Ensemble â€“ (3) Boosting

> converts weak learner to strong learners.

 !["Boosting"](./image/7.jpg)

boostingì˜ ì•„ì´ë””ì–´ëŠ” weak learnerë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ”ë°, ì˜ëª»ëœ ë¶€ë¶„ì„ ê³ ì¹˜ë©´ì„œ ì§„í–‰í•œë‹¤.
ì´ì „ ëª¨ë¸ì€ ë‹¤ìŒ ëª¨ë¸ì—ê²Œ ì–´ë–¤ featureì— ì£¼ëª©í•´ì•¼ í•˜ëŠ” ì§€ ì•Œë ¤ì¤€ë‹¤.
 

> 1. Boosting â€“ AdaBoost

boostingì˜ ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ DTê°€ modelingì„ ìœ„í•´ ì‚¬ìš©ëœë‹¤.
ì—¬ëŸ¬ ê°œì˜ ìˆœì°¨ì ì¸ ëª¨ë¸ì´ ë§Œë“¤ì–´ì§€ê³ , ê°ê°ì€ ì§€ë‚œ ëª¨ë¸ì˜ ì—ëŸ¬ë¥¼ correctingí•œë‹¤.
AdaBoostëŠ” ì˜ëª» ì˜ˆì¸¡ëœ ê´€ì¸¡ì¹˜ì— weightë¥¼ ë¶€ì—¬í•˜ê³ , subsequent modelëŠ” ì´ ê°’ì„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ë™ì‘í•œë‹¤.

1. ë°ì´í„°ì…‹ì˜ ëª¨ë“  ê´€ì¸¡ì¹˜ê°€ ë™ì¼í•œ weightë¥¼ ë¶€ì—¬ë°›ëŠ”ë‹¤.
2. **ëª¨ë¸ì€ dataì˜ subsetìœ¼ë¡œ í•™ìŠµëœë‹¤.**
3. ì´ ëª¨ë¸ì„ ì´ìš©í•´ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì§„í–‰í•œë‹¤.
4. ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì„ ë¹„êµí•˜ì—¬ ì—ëŸ¬ê°€ ê³„ì‚°ëœë‹¤.
5. ë‹¤ìŒ ëª¨ë¸ì„ ë§Œë“¤ë©°, **ì˜ëª» ì˜ˆì¸¡ëœ ë°ì´í„°ì—ëŠ” ë” ë§ì€ ê°€ì¤‘ì¹˜**ê°€ ë¶€ì—¬ëœë‹¤.
6. ê°€ì¤‘ì¹˜ëŠ” error valueì— ë”°ë¼ì„œ ê²°ì •ëœë‹¤. (errorê°€ ë†’ì„ìˆ˜ë¡ ê·¸ ê´€ì¸¡ì¹˜ì— weightëŠ” ì¦ê°€)
7. ì´ ê³¼ì •ì€ error functionì´ ë” ì´ìƒ ë³€í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µë˜ê±°ë‚˜ ì œí•œëœ estimatorsì˜ ìˆ˜ë§Œí¼ ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µëœë‹¤.

AdaBoostì—ì„œ ê¸°ì–µí•´ì•¼ í•  ê²ƒ
1. ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ë†’ê²Œ ë¶€ì—¬í•œë‹¤ëŠ” ê²ƒ
2. ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ modelì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì¶˜ë‹¤ëŠ” ê²ƒ
**AdaBoostì—ì„œëŠ” 1-depth decision tree (=stump)ë¥¼ ì‚¬ìš©í•œë‹¤.**

 >
 !["AdaBoost"](./image/6.jpg)
ì ˆì°¨
1. ëª¨ë“  instanceì˜ initial weightëŠ” ë™ì¼í•˜ê²Œ ì§€ì • 
2. instanceì˜ weight ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ samplingì„ ì§„í–‰
3. ë¶„ë¥˜ê¸°ì˜ ì—ëŸ¬ë¥¼ ê³„ì‚° (í‹€ë¦° ë°ì´í„°ì˜ weightì˜ í•© / ì „ì²´ weightì˜ í•©)
4. log((1-err)/err)ê°€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ëœë‹¤ (errorì˜ ì •ë„ì™€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ë°˜ë¹„ë¡€)
5. í‹€ë¦° instanceì˜ ê²½ìš° weightë¥¼ ì—…ë°ì´íŠ¸ (weightê°€ ë” ì»¤ì§€ë„ë¡)

+ **ì—¬ê¸°ì„œ baggingê³¼ boostingì˜ ì°¨ì´ì— ëŒ€í•´ ì •ë¦¬**
baggingì€ ê°ê°ì˜ ëª¨ë¸ì´ ë…ë¦½ì  (bootstrappingí•œ subsample ë°ì´í„° ì´ìš©í•´ ê°ê° í•™ìŠµí•œ ê²°ê³¼ë¥¼ ì·¨í•©)
boostingì€ ê°ê°ì˜ ëª¨ë¸ì´ ì´ì „ ëª¨ë¸ì— ì˜ì¡´ì  (ì´ì „ ëª¨ë¸ì—ì„œ ì˜ëª» ì˜ˆì¸¡í•œ instanceì˜ ê²½ìš° ë” ì£¼ëª©í•˜ê²Œ)
baggingê³¼ boosting ëª¨ë‘ dataì—ì„œ samplingí•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•œë‹¤ëŠ” ê²ƒì€ ê³µí†µì 
 
 

ê°œë³„ decision treeì˜ ì„±ëŠ¥ì´ ë‚®ë‹¤ë©´ -> boosting
overfitting ë¬¸ì œë¥¼ ë§‰ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ë©´ -> bagging

 
â€ƒ
> 2. Boosting â€“ Gradient Boosting

**ì´ì „ì˜ residualì„ ê°€ì§€ê³  modelë¥¼ ê°•í™”í•˜ëŠ” ë°©ì‹ + decision tree ê¸°ë°˜ ëª¨ë¸**
**(tree1ì„ í†µí•´ ë‚¨ì€ residualì„ tree2ê°€ ì˜ˆì¸¡í•˜ê³ , ë˜ ë‚¨ì€ residualì„ tree3ê°€ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹)**
ì¼ë°˜ì ìœ¼ë¡œ terminal leafì˜ ìˆ˜ëŠ” data sizeì— ë”°ë¼ 4, 8, 32ê°œë¡œ ì œí•œí•œë‹¤. (AdaBoostë³´ë‹¤ëŠ” ëœ ì œí•œì )
(ì´ì „ì˜ errorì— ê¸°ë°˜í•˜ì—¬ decision tree ìƒì„±)
scale the tree by learning rate
subsampling í•˜ëŠ” ë°©ì‹ì€ random forestì™€ ë™ì¼í•˜ë‹¤.

 !["GBM"](./image/8.jpg)
 
ì—¬ê¸°ì„œ Lossë¥¼ SSE/2ë¡œ ì •ì˜í•˜ë©´ lossì˜ negative gradientëŠ” residualì´ ëœë‹¤.
ì—¬ê¸°ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê²ƒê³¼ ê°™ì´ negative gradientë¥¼ ì´ìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— GBM
f1(x) = f2(x) + alpha*residual,
f2(x) = f3(x) + alpha*residual,
f3(x) = f4(x) + alpha*residual, â€¦ 
f1(x) = f2(x) + f2(x) + f4(x) + â€¦ 
 

!["boosting"](./image/9.png)
ì—¬ê¸°ì„œ ìš°ì„  ì²« ë²ˆì§¸ ì˜ˆì¸¡ê°’ì€ targetì˜ averageë¡œ ì„¤ì •í•œë‹¤. 
ì²˜ìŒì— í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê¸°ë¡œ í–ˆìœ¼ë¯€ë¡œ ì´ì œëŠ” ì”ì°¨ë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.
(ì‹¤ì œê°’-í‰ê· ê°’)ì„ í†µí•´ residualì„ êµ¬í•œë‹¤.
ê·¸ë¦¬ê³  ì—¬ëŸ¬ê°€ì§€ì˜ featureë¥¼ í†µí•´ **residualì„ ì˜ˆì¸¡í•˜ëŠ” decision tree**ë¥¼ ë§Œë“ ë‹¤.
ì´ ë•Œ ë°ì´í„°ê°€ ì‘ìœ¼ë©´ terminal nodeì˜ ìˆ˜ë¥¼ 4ê°œ, ë°ì´í„°ì˜ ìˆ˜ê°€ í¬ë©´ 8, 16, 32ê°œë¡œ ì„¤ì •í•œë‹¤.
ë˜í•œ targetì´ ì—°ì†í˜•ì¸ ê²½ìš° terminal nodeì˜ ê°’ì€ ê·¸ nodeì— ë“¤ì–´ê°€ëŠ” ë°ì´í„°ì˜ targetì˜ (min+max)/2ë¡œ ì„¤ì •í•œë‹¤.
ì—¬ê¸°ì„œ ë©ˆì¶°ì„œ í‰ê· ê°’ + residualë¡œ ìµœì¢… ì˜ˆì¸¡ ê°’ì„ ê²°ì •í•˜ê²Œ ë˜ë©´ **training ë°ì´í„°ì— ë§¤ìš° overfitting** â˜¹ ëœë‹¤. (high variance)
**ë”°ë¼ì„œ GBMì€ ì´ ë¬¸ì œë¥¼ treeì˜ í¬ê¸°ë¥¼ scalingí•˜ì—¬ í•´ê²°í•˜ëŠ”ë°, ì´ ë•Œ learning rateë¥¼ ì‚¬ìš©í•œë‹¤.**
-> ì˜ˆì¸¡ê°’ì„ â€˜í‰ê· ê°’ + learning_rate*residualâ€™ìœ¼ë¡œ ì„¤ì • 
(**learning rate ì´ìš©í•˜ì—¬ ì •ë‹µì— ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë„ë¡**)
(ì´ë ‡ê²Œ small steps in the right directionì€ test dataì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚˜íƒ€ëƒˆë‹¤ê³  í•¨ ğŸ˜Š)
ì´ì œ ë‹¤ì‹œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’(í‰ê· +learning_rate*residual)ì„ ë¹„êµí•˜ì—¬ new_residualì„ êµ¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. (+ í‰ê· ê°’ìœ¼ë¡œ ê·¸ëƒ¥ ì˜ˆì¸¡í•˜ì—¬ ì”ì°¨ë¥¼ êµ¬í•œ ê²ƒë³´ë‹¤ í‰ê· ê°’+learning_rate*residualë¡œ ì˜ˆì¸¡í•˜ì—¬ ì”ì°¨ë¥¼ êµ¬í•œ ê²Œ ë” ì‘ìŒ -> ì¦‰ ì •ë‹µì— ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ê³  ìˆë‹¤ëŠ” ì˜ë¯¸)
**ì´ì œ ë‹¤ì‹œ new_residualì„ ì˜ˆì¸¡í•˜ëŠ” new_decision treeë¥¼ ë§Œë“ ë‹¤.**
ì´ì œëŠ” **í‰ê·  + learning_rate*tree1 + learning_rate*tree2ì˜ ê°’ìœ¼ë¡œ ì˜ˆì¸¡**í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.
ì—¬ê¸°ì„œ ë‹¤ì‹œ new_residaulì„ êµ¬í•˜ê³ , ë‹¤ì‹œ ì´ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” decision treeë¥¼ ë§Œë“¤ê³  â€¦
ì´ë ‡ê²Œ ì„¤ì •ëœ ê°œìˆ˜ë§Œí¼ treeë¥¼ ë§Œë“¤ ë•Œê¹Œì§€, í˜¹ì€ ë” ì´ìƒ ì”ì°¨ê°€ ì¤„ì–´ë“¤ ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.


> 3. Boosting â€“ XGBoost (GBM + tree ê¸°ë°˜)

GBMì€ ì„±ëŠ¥ì€ ì¢‹ìœ¼ë‚˜ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦°ë‹¤ â˜¹ -> ë³´ì™„í•œ ê²ƒì´ XGBoost
+ GBMì€ train dataì—ì„œì˜ residualì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê¸° ë•Œë¬¸ì— ì‰½ê²Œ overfitting â˜¹
ë”°ë¼ì„œ GBMì— regularizationì„ ì¶”ê°€í•œ ê²ƒì´ XGBoostì´ë‹¤.
**ê· í˜• íŠ¸ë¦¬ ë¶„í•  (level wise)**

> 4. Boosting â€“ LightGBM (GBM + tree ê¸°ë°˜)

**ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  (leaf wise)** â€“ ë¹„ê· í˜• ì´ë”ë¼ë„ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ë¶„í• 
(ì´ì „ë³´ë‹¤ ì†ë„ í–¥ìƒ)

ê· í˜• ì¡íŒ íŠ¸ë¦¬ëŠ” overfittingì— ë³´ë‹¤ robustí•˜ë‹¤ëŠ” ì¥ì 
ë¦¬í”„ ì¤‘ì‹¬ íŠ¸ë¦¬ë¶„í•  ë°©ì‹ì€ íŠ¸ë¦¬ì˜ ê· í˜•ì„ ë§ì¶”ì§€ ì•Šê³ , ìµœëŒ€ì†ì‹¤ ê°’ì„ ê°€ì§€ëŠ” ë¦¬í”„ë…¸ë“œë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¶„í• 

 
---

## Ensemble â€“ (4) Stacking

ì‰½ê²Œ ë§í•´ì„œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ì„ meta modelì´ ë‹¤ì‹œ í•™ìŠµí•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ í•˜ëŠ” ê²ƒ
 
 !["stacking"](./image/10.jpg)
+ CV ê¸°ë°˜ stacking
https://lsjsj92.tistory.com/559?category=853217
1. dataë¥¼ X_train(n1*k), X_test(n2*k), y_train, y_test ë°ì´í„°ë¡œ ë¶„ë¦¬
2. training dataë¥¼ k-Foldë¡œ ë‚˜ëˆˆë‹¤. (ì—¬ê¸°ì„œëŠ” 5ë¼ê³  ê°€ì •)
3. ëª¨ë¸ì€ 4ì¡°ê°ì˜ ë°ì´í„°ë¡œ í•™ìŠµ ì§„í–‰ + 1ì¡°ê°ì˜ validation dataë¥¼ ì˜ˆì¸¡
ì´ë ‡ê²Œ ê°ê°ì˜ validation dataë¥¼ ì˜ˆì¸¡í•˜ë©´ 5ê°œì˜ ì˜ˆì¸¡ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤ (ì•„ë˜ë¡œ ì´ì–´ë¶™ì¸ë‹¤)
ì´ê²ƒì„ meta modelì˜ training dataë¡œ ì‚¬ìš© (n1*1)
4. test ë°ì´í„°ë¥¼ ê° ê°œë³„ ëª¨ë¸ì— ë„£ì–´ predicted valueë¥¼ êµ¬í•¨ -> 5ê°œì˜ predicted value
-> ì´ ê°’ì„ í‰ê· ë‚´ì–´ meta modelì˜ test dataë¡œ ì‚¬ìš© (n2*1)
ë§Œì•½ì— mê°œì˜ ëª¨ë¸ì´ë¼ë©´ (n1*1)ì´ mê°œ ìƒê¸´ë‹¤ -> ì´ê²ƒì„ ì˜†ìœ¼ë¡œ ì´ì–´ë¶™ì¸ë‹¤ (n1*m) = new_train
ë˜í•œ test ë°ì´í„°ë¡œ (n2*1)ì´ mê°œ ìƒê¸´ë‹¤ -> ì´ê²ƒì„ ì˜†ìœ¼ë¡œ ì´ì–´ë¶™ì¸ë‹¤ (n2*m) = new_test
5. meta modelì—ì„œëŠ” new_trainì„ X_trainì²˜ëŸ¼, y_trainì„ y_trainìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ
-> new_testë¥¼ X_testì²˜ëŸ¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ëœ ê²°ê³¼ê°’ì„ ìµœì¢… predict ê°’ìœ¼ë¡œ ì‚¬ìš©
ìµœì¢… test scoreëŠ” X_testë¥¼ ì˜ˆì¸¡í•œ ê²°ê³¼ì™€ y_testë¥¼ ë¹„êµ

---

## Summary
ensemble
1. voting (max voting(hard, soft), averaging, weight average)
2. bagging (bagging meta-classifier, random forest)
3. boosting (AdaBoost, gradient boosting, XGBoost, LightGBM)
4. stacking

ë³´íŒ…ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ê°€ì§„ ë¶„ë¥˜ê¸°ê°€ ê°™ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ í•œ í›„ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ëŠ” ê²ƒ
ë°°ê¹…ì˜ ê²½ìš° ì¼ë°˜ì ìœ¼ë¡œ ê°™ì€ ìœ í˜•ì˜ ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ì´ì§€ë§Œ, ë°ì´í„° ìƒ˜í”Œë§ì„ ì„œë¡œ ë‹¤ë¥´ê²Œ í•˜ë©´ì„œ(=ì›ë³¸ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§ì„ í•˜ëŠ”ë°, ê°ê°ì˜ í•™ìŠµê¸°ê°€ ìƒ˜í”Œë§ëœ ë°ì´í„°ë¥¼ ê°ì í•™ìŠµ) í•™ìŠµì„ ìˆ˜í–‰í•œ í›„ ë³´íŒ…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒ
RFëŠ” ì—¬ëŸ¬ ê°œì˜ DTê°€ ì „ì²´ ë°ì´í„°ì—ì„œ bootstrappingìœ¼ë¡œ ê°ìì˜ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•´ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµì„ ìˆ˜í–‰í•œ ë’¤ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  ë¶„ë¥˜ê¸°ê°€ ë³´íŒ…(soft)ì„ í†µí•´ ì˜ˆì¸¡ ê²°ì •ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.
ë¶€ìŠ¤íŒ…ì€ decision tree ê¸°ë°˜ì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, ë°ì´í„° ìƒ˜í”Œë§ì„ ì„œë¡œ ë‹¤ë¥´ê²Œ í•˜ë©´ì„œ í•™ìŠµì„ ìˆ˜í–‰í•œë‹¤. ì´ ë•Œ ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ì˜ ê²½ìš° weightë¥¼ ë†’ê²Œ ë¶€ì—¬í•˜ì—¬ ìƒ˜í”Œë§ì´ ë” ì˜ ë˜ë„ë¡ í•œë‹¤. ì´ë ‡ê²Œ ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ì— ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì´ìœ ëŠ” ì´ ë°ì´í„°ì— ì¢€ ë” ì§‘ì¤‘í•˜ì—¬ ë” ì˜ ë¶„ë¥˜í•´ë³´ìëŠ” ì•„ì´ë””ì–´ì—ì„œ ë¹„ë¡¯ë˜ì—ˆë‹¤.
ìŠ¤íƒœí‚¹ì€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ meta ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ë°©ë²•ì´ë‹¤.

---

#### reference
- https://www.kaggle.com/amrmahmoud123/1-guide-to-ensembling-methods
- https://3months.tistory.com/368 (gradient boosting)
- https://www.youtube.com/watch?v=3CC4N4z3GJc (gradient boosting)
- ë¨¸ì‹ ëŸ¬ë‹ ì™„ë²½ ê°€ì´ë“œ
